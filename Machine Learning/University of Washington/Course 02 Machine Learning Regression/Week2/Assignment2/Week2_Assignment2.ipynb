{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week2 Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs7onhfaVyL1",
        "colab_type": "text"
      },
      "source": [
        "#**Regression Week 2: Multiple Regression (gradient descent)**\n",
        "In the first notebook we explored multiple regression using graphlab create. Now we will use graphlab along with numpy to solve for the regression weights with gradient descent.\n",
        "\n",
        "In this notebook we will cover estimating multiple regression weights via gradient descent. You will:\n",
        "\n",
        "\n",
        "*   Add a constant column of 1's to a graphlab SFrame to     account for the intercept\n",
        "\n",
        "*   Convert an SFrame into a Numpy array\n",
        "\n",
        "*   Write a predict_output() function using Numpy\n",
        "\n",
        "*   Write a numpy function to compute the derivative of the regression weights with respect to a single feature\n",
        "\n",
        "\n",
        "*   Write gradient descent function to compute the regression \n",
        "\n",
        "*   weights given an initial weight vector, step size and tolerance.\n",
        "\n",
        "*   Use the gradient descent function to estimate regression weights for multiple features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBMJ1kYAVLPq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b8939b7-ae3a-4840-c058-043688a8c25d"
      },
      "source": [
        "!pip install turicreate"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting turicreate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/76/76c624d7ae1116b22cd559288596a1f9aa7a50f8f43f4481033fc047f5e9/turicreate-6.3-cp36-cp36m-manylinux1_x86_64.whl (91.9MB)\n",
            "\u001b[K     |████████████████████████████████| 91.9MB 57kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from turicreate) (1.18.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from turicreate) (1.4.1)\n",
            "Collecting tensorflow<=2.0.1,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/16/b07e3f7a4a024b47918f7018967eb984b0c542458a6141d8c48515aa81d4/tensorflow-2.0.1-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 63kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.23.2 in /usr/local/lib/python3.6/dist-packages (from turicreate) (1.0.4)\n",
            "Requirement already satisfied: decorator>=4.0.9 in /usr/local/lib/python3.6/dist-packages (from turicreate) (4.4.2)\n",
            "Requirement already satisfied: pillow>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from turicreate) (7.0.0)\n",
            "Requirement already satisfied: prettytable==0.7.2 in /usr/local/lib/python3.6/dist-packages (from turicreate) (0.7.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from turicreate) (1.12.0)\n",
            "Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.6/dist-packages (from turicreate) (2.23.0)\n",
            "Collecting resampy==0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/b6/66a06d85474190b50aee1a6c09cdc95bb405ac47338b27e9b21409da1760/resampy-0.2.1.tar.gz (322kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 38.2MB/s \n",
            "\u001b[?25hCollecting coremltools==3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/19/611916d1ef326d38857d93af5ba184f6ad7491642e0fa4f9082e7d82f034/coremltools-3.3-cp36-none-manylinux1_x86_64.whl (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 32.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=2.0.1,>=2.0.0->turicreate) (0.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=2.0.1,>=2.0.0->turicreate) (1.1.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=2.0.1,>=2.0.0->turicreate) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=2.0.1,>=2.0.0->turicreate) (0.8.1)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=2.0.1,>=2.0.0->turicreate) (1.12.1)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 40.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=2.0.1,>=2.0.0->turicreate) (1.29.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=2.0.1,>=2.0.0->turicreate) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<=2.0.1,>=2.0.0->turicreate) (0.34.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=2.0.1,>=2.0.0->turicreate) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=2.0.1,>=2.0.0->turicreate) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<=2.0.1,>=2.0.0->turicreate) (3.2.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.2->turicreate) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.2->turicreate) (2.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->turicreate) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->turicreate) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->turicreate) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->turicreate) (1.24.3)\n",
            "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.6/dist-packages (from resampy==0.2.1->turicreate) (0.48.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (47.1.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (1.7.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow<=2.0.1,>=2.0.0->turicreate) (2.10.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.32->resampy==0.2.1->turicreate) (0.31.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (1.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (3.1.0)\n",
            "Building wheels for collected packages: resampy, gast\n",
            "  Building wheel for resampy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for resampy: filename=resampy-0.2.1-cp36-none-any.whl size=320850 sha256=09cbd483d6384ee62acf787697620ffd030c890f2f89068188c22116975d3b7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/4f/ed/2e6c676c23efe5394bb40ade50662e90eb46e29b48324c5f9b\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=18d6e1ac9e20a87d3cdd6cf2608e6549b76a5f7af84fd8b6319d38f57513e4b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built resampy gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, tensorflow-estimator, tensorboard, tensorflow, resampy, coremltools, turicreate\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "  Found existing installation: resampy 0.2.2\n",
            "    Uninstalling resampy-0.2.2:\n",
            "      Successfully uninstalled resampy-0.2.2\n",
            "Successfully installed coremltools-3.3 gast-0.2.2 resampy-0.2.1 tensorboard-2.0.2 tensorflow-2.0.1 tensorflow-estimator-2.0.1 turicreate-6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYTN3HYtVR8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import turicreate\n",
        "from turicreate import SFrame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIPznEuRWpAI",
        "colab_type": "text"
      },
      "source": [
        "#**Load in house sales data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li1fyddTVthl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sales = turicreate.SFrame('/content/drive/My Drive/Colab Notebooks/Machine Learning/Course 2 Machine Learning Regression/Week2/Assignment1/home_data.sframe')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPnxofrTXG2c",
        "colab_type": "text"
      },
      "source": [
        "2. If you’re using python: to do the matrix operations required to perform a gradient descent we will be using the popular python library ‘numpy’ which is a computational library specialized for operations on arrays. For students unfamiliar with numpy we have created a numpy tutorial (see useful resources). It is common to import numpy under the name ‘np’ for short, to do this execute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBGF5YKNW-ju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmA0H2w1XNqP",
        "colab_type": "text"
      },
      "source": [
        "3. Next write a function that takes a data set, a list of features \n",
        "  \n",
        "         (e.g. [‘sqft_living’, ‘bedrooms’])\n",
        "  \n",
        "   to be used as inputs, and a name of the output (e.g. ‘price’). This function should return a features_matrix (2D array) consisting of first a column of ones followed by columns containing the values of the input features in the data set in the same order as the input list. It should also return an output_array which is an array of the values of the output in the data set (e.g. ‘price’). e.g. if you’re using SFrames and numpy you can complete the following function:\n",
        "\n",
        "       def get_numpy_data(data_sframe, features, output):\n",
        "            data_sframe['constant'] = 1 # add a constant column to an SFrame\n",
        "            # prepend variable 'constant' to the features list\n",
        "            features = ['constant'] + features\n",
        "            # select the columns of data_SFrame given by the ‘features’ list into the SFrame ‘features_sframe’\n",
        "\n",
        "            # this will convert the features_sframe into a numpy matrix:\n",
        "            features_matrix = features_sframe.to_numpy()\n",
        "            # assign the column of data_sframe associated with the target to the variable ‘output_sarray’\n",
        "\n",
        "            # this will convert the SArray into a numpy array:\n",
        "            output_array = output_sarray.to_numpy()\n",
        "            return(features_matrix, output_array)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDdZFg8lXKQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_numpy_data(data_sframe, features, output):\n",
        "\n",
        "  data_sframe['constant'] = 1 # add a constant column to an SFrame\n",
        "  # prepend variable 'constant' to the features list\n",
        "  features = ['constant'] + features\n",
        "  # select the columns of data_SFrame given by the ‘features’ list into the SFrame ‘features_sframe’\n",
        "  features_sframe = data_sframe[features]\n",
        "  # this will convert the features_sframe into a numpy matrix:\n",
        "  features_matrix = features_sframe.to_numpy()\n",
        "  # assign the column of data_sframe associated with the target to the variable ‘output_sarray’\n",
        "  output_sarray = data_sframe[output]\n",
        "  # this will convert the SArray into a numpy array:\n",
        "  output_array = output_sarray.to_numpy()\n",
        "  return(features_matrix, output_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuSgEx-lYO1c",
        "colab_type": "text"
      },
      "source": [
        "For testing let's use the 'sqft_living' feature and a constant as our features and price as our output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoylXioLYIME",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "05be56b2-39dc-46d9-8437-e1bc942b466b"
      },
      "source": [
        "(example_features, example_output) = get_numpy_data(sales, ['sqft_living'], 'price') # the [] around 'sqft_living' makes it a list\n",
        "print(example_features[0,:]) # this accesses the first row of the data the ':' indicates 'all columns'\n",
        "print(example_output[0]) # and the corresponding output"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.00e+00 1.18e+03]\n",
            "221900.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRApIpkKZevT",
        "colab_type": "text"
      },
      "source": [
        "4. If the features matrix (including a column of 1s for the constant) is stored as a 2D array (or matrix) and the regression weights are stored as a 1D array then the predicted output is just the dot product between the features matrix and the weights (with the weights on the right). Write a function ‘predict_output’ which accepts a 2D array ‘feature_matrix’ and a 1D array ‘weights’ and returns a 1D array ‘predictions’. e.g. in python:\n",
        "\n",
        "        def predict_outcome(feature_matrix, weights):\n",
        "           [your code here]\n",
        "           return(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuLgK068YXx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_output(feature_matrix, weights):\n",
        "    # assume feature_matrix is a numpy matrix containing the features as columns \n",
        "    # and weights is a corresponding numpy array\n",
        "    # create the predictions vector by using np.dot()\n",
        "    predictions = np.dot(feature_matrix, weights)\n",
        "    return(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkSo5K7NaRkA",
        "colab_type": "text"
      },
      "source": [
        "to test our code run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg8aPUOqaMH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_weights = np.array([1., 1.]) # the example weights\n",
        "my_features = example_features[0,] # we'll use the first data point"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpD-Rtmlaiv-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4b48236c-0c53-4c2b-d8bf-ca28105d34ea"
      },
      "source": [
        "test_predictions = predict_output(example_features, my_weights)\n",
        "print(test_predictions[0]) \n",
        "print(test_predictions[1]) "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1181.0\n",
            "2571.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5E2rbINaufl",
        "colab_type": "text"
      },
      "source": [
        "5. If we have a the values of a single input feature in an array ‘feature’ and the prediction ‘errors’ (predictions - output) then the derivative of the regression cost function with respect to the weight of ‘feature’ is just twice the dot product between ‘feature’ and ‘errors’. Write a function that accepts a ‘feature’ array and ‘error’ array and returns the ‘derivative’ (a single number). e.g. in python:\n",
        "\n",
        "    \n",
        "        def feature_derivative(errors, feature):\n",
        "            [your code here]\n",
        "            return(derivative)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtlBEo_zbGXf",
        "colab_type": "text"
      },
      "source": [
        "We are now going to move to computing the derivative of the regression cost function. Recall that the cost function is the sum over the data points of the squared difference between an observed output and a predicted output.\n",
        "\n",
        "Since the derivative of a sum is the sum of the derivatives we can compute the derivative for a single data point and then sum over data points. We can write the squared difference between the observed output and predicted output for a single point as follows:\n",
        "\n",
        "(w[0]*[CONSTANT] + w[1]*[feature_1] + ... + w[i] *[feature_i] + ... + w[k]*[feature_k] - output)^2\n",
        "\n",
        "Where we have k features and a constant. So the derivative with respect to weight w[i] by the chain rule is:\n",
        "\n",
        "2*(w[0]*[CONSTANT] + w[1]*[feature_1] + ... + w[i] *[feature_i] + ... + w[k]*[feature_k] - output)* [feature_i]\n",
        "\n",
        "The term inside the paranethesis is just the error (difference between prediction and output). So we can re-write this as:\n",
        "\n",
        "2*error*[feature_i]\n",
        "\n",
        "That is, the derivative for the weight for feature i is the sum (over data points) of 2 times the product of the error and the feature itself. In the case of the constant then this is just twice the sum of the errors!\n",
        "\n",
        "Recall that twice the sum of the product of two vectors is just twice the dot product of the two vectors. Therefore the derivative for the weight for feature_i is just two times the dot product between the values of feature_i and the current errors.\n",
        "\n",
        "With this in mind complete the following derivative function which computes the derivative of the weight given the value of the feature (over all data points) and the errors (over all data points)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWkxaPryb2w-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_derivative(errors,feature):\n",
        "  # Assume that errors and feature are both numpy arrays of the same length (number of data points)\n",
        "  # compute twice the dot product of these vectors as 'derivative' and return the value\n",
        "  derivative = 2*np.dot(errors, feature)\n",
        "  return(derivative)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDUr4wJ4bRQQ",
        "colab_type": "text"
      },
      "source": [
        "To test our feature derivartive run the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObWJRLJHarDh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d8d0143f-afd3-45c8-dfc0-e5bc8646ad17"
      },
      "source": [
        "(example_features, example_output) = get_numpy_data(sales, ['sqft_living'], 'price') \n",
        "my_weights = np.array([0., 0.]) # this makes all the predictions 0\n",
        "test_predictions = predict_output(example_features, my_weights) \n",
        "# just like SFrames 2 numpy arrays can be elementwise subtracted with '-': \n",
        "errors = test_predictions - example_output # prediction errors in this case is just the -example_output\n",
        "feature = example_features[:,0] # let's compute the derivative with respect to 'constant', the \":\" indicates \"all rows\"\n",
        "derivative = feature_derivative(errors, feature)\n",
        "print(derivative)\n",
        "print(-np.sum(example_output)*2)  #should be the same as derivative"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-23345850022.0\n",
            "-23345850022.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POCU4gCgeSla",
        "colab_type": "text"
      },
      "source": [
        "6. Now we will use our predict_output and feature_derivative to write a gradient descent function. Although we can compute the derivative for all the features simultaneously (the gradient) we will explicitly loop over the features individually for simplicity. Write a gradient descent function that does the following:\n",
        "\n",
        "Accepts a numpy feature_matrix 2D array, a 1D output array, an array of initial weights, a step size and a convergence tolerance.\n",
        "While not converged updates each feature weight by subtracting the step size times the derivative for that feature given the current weights\n",
        "At each step computes the magnitude/length of the gradient (square root of the sum of squared components)\n",
        "When the magnitude of the gradient is smaller than the input tolerance returns the final weight vector.\n",
        "e.g. if you’re using SFrames and numpy you can complete the following function:\n",
        "\n",
        "        def regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance):\n",
        "            converged = False\n",
        "            weights = np.array(initial_weights)\n",
        "            while not converged:\n",
        "                # compute the predictions based on feature_matrix and weights:\n",
        "                # compute the errors as predictions - output:\n",
        "                \n",
        "                gradient_sum_squares = 0 # initialize the gradient\n",
        "                # while not converged, update each weight individually:\n",
        "                for i in range(len(weights)):\n",
        "                    # Recall that feature_matrix[:, i] is the feature column associated with weights[i]\n",
        "                    # compute the derivative for weight[i]:\n",
        "                    \n",
        "                    # add the squared derivative to the gradient magnitude\n",
        "                    \n",
        "                    # update the weight based on step size and derivative:\n",
        "                    \n",
        "                gradient_magnitude = sqrt(gradient_sum_squares)\n",
        "                if gradient_magnitude < tolerance:\n",
        "                    converged = True\n",
        "            return(weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsKVg0gGjj0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from math import sqrt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmRPk2-QbX82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance):\n",
        "    converged = False \n",
        "    weights = np.array(initial_weights) # make sure it's a numpy array\n",
        "    while not converged:\n",
        "        # compute the predictions based on feature_matrix and weights using your predict_output() function\n",
        "        predictions = predict_output(feature_matrix, weights)\n",
        "        # compute the errors as predictions - output\n",
        "        errors = predictions - output\n",
        "        gradient_sum_squares = 0 # initialize the gradient sum of squares\n",
        "        # while we haven't reached the tolerance yet, update each feature's weight\n",
        "        for i in range(len(weights)): # loop over each weight\n",
        "            # Recall that feature_matrix[:, i] is the feature column associated with weights[i]\n",
        "            # compute the derivative for weight[i]:\n",
        "            feature = feature_matrix[:, i]\n",
        "            derivative = feature_derivative(errors, feature)\n",
        "            # add the squared value of the derivative to the gradient sum of squares (for assessing convergence)\n",
        "            gradient_sum_squares += derivative**2\n",
        "            # subtract the step size times the derivative from the current weight\n",
        "            weights[i] -= step_size*derivative\n",
        "        # compute the square-root of the gradient sum of squares to get the gradient matnigude:\n",
        "        gradient_magnitude = sqrt(gradient_sum_squares)\n",
        "        if gradient_magnitude < tolerance:\n",
        "            converged = True\n",
        "    return(weights)\n",
        "\n",
        "#A few things to note before we run the gradient descent. \n",
        "#Since the gradient is a sum over all the data points and involves a product of an error \n",
        "#and a feature the gradient itself will be very large since the features are large (squarefeet) \n",
        "#and the output is large (prices). So while you might expect \"tolerance\" to be small, \n",
        "#small is only relative to the size of the features.\n",
        "\n",
        "#For similar reasons the step size will be much smaller than you might expect \n",
        "#but this is because the gradient has such large values."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_UIsYCUfzWW",
        "colab_type": "text"
      },
      "source": [
        "7. Now split the sales data into training and test data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ0ZfEfUfp58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, test_data = sales.random_split(.8, seed=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GmCKZougcQ9",
        "colab_type": "text"
      },
      "source": [
        "8. Now we will run the regression_gradient_descent function on some actual data. In particular we will use the gradient descent to estimate the model from Week 1 using just an intercept and slope. Use the following parameters:\n",
        "\n",
        "features: ‘sqft_living’\n",
        "\n",
        "output: ‘price’\n",
        "\n",
        "initial weights: -47000, 1 (intercept, sqft_living respectively)\n",
        "\n",
        "step_size = 7e-12\n",
        "\n",
        "tolerance = 2.5e7\n",
        "\n",
        "e.g. in python with numpy and SFrames:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tViPjC01gRYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simple_features = ['sqft_living']\n",
        "my_output= 'price'\n",
        "(simple_feature_matrix, output) = get_numpy_data(train_data, simple_features, my_output)\n",
        "initial_weights = np.array([-47000., 1.])\n",
        "step_size = 7e-12\n",
        "tolerance = 2.5e7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60WGB0R1gvxI",
        "colab_type": "text"
      },
      "source": [
        "Use these parameters to estimate the slope and intercept for predicting prices based only on ‘sqft_living’."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQhgJGORgntd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce3526dc-d20f-4d58-bf6a-bc304fe51173"
      },
      "source": [
        "my_sqft_weights = regression_gradient_descent(simple_feature_matrix, output, initial_weights, step_size, tolerance)\n",
        "print(my_sqft_weights)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-46999.88716555    281.91211912]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z_DwTxLCVvR",
        "colab_type": "text"
      },
      "source": [
        "#9. **Quiz Question:** What is the value of the weight for sqft_living -- the second element of ‘simple_weights’ (rounded to 1 decimal place)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz7qnDMjDCYe",
        "colab_type": "text"
      },
      "source": [
        "10. Now build a corresponding ‘test_simple_feature_matrix’ and ‘test_output’ using test_data. Using ‘test_simple_feature_matrix’ and ‘simple_weights’ compute the predicted house prices on all the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Szp2r7hgz25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_simple_feature_matrix, test_output = get_numpy_data(test_data, simple_features, my_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTV_PAlNDQd0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e5bc750a-201b-4868-dad8-dc6a7afb6b42"
      },
      "source": [
        "house_price_prediction = predict_output(test_simple_feature_matrix, my_sqft_weights)\n",
        "print('House price prediction : ', house_price_prediction)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "House price prediction :  [356134.44317093 784640.86422788 435069.83652353 ... 663418.65300782\n",
            " 604217.10799338 240550.4743332 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fg3WGP9EFLT",
        "colab_type": "text"
      },
      "source": [
        "#11. **Quiz Question:** What is the predicted price for the 1st house in the Test data set for model 1 (round to nearest dollar)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBYZSnatD8Z9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dca6b5b9-c6c6-4d4c-9121-6b383d4a1a74"
      },
      "source": [
        "print(np.rint(house_price_prediction[0]))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "356134.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O1hsHFbEcWv",
        "colab_type": "text"
      },
      "source": [
        "12. Now compute RSS on all test data for this model. Record the value and store it for later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKe35W_PEXIo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "010861e4-38b1-41ec-b7a9-03060cce1351"
      },
      "source": [
        "test_errors = house_price_prediction - test_output\n",
        "RSS = (test_errors*test_errors).sum()\n",
        "print('RSS : ', RSS)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RSS :  275400047593155.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS1976pxGDSp",
        "colab_type": "text"
      },
      "source": [
        "13. Now we will use the gradient descent to fit a model with more than 1 predictor variable (and an intercept). Use the following parameters:\n",
        "\n",
        "model features = ‘sqft_living’, ‘sqft_living_15’\n",
        "\n",
        "output = ‘price’\n",
        "\n",
        "initial weights = [-100000, 1, 1] (intercept, sqft_living, and sqft_living_15 respectively)\n",
        "\n",
        "step size = 4e-12\n",
        "\n",
        "tolerance = 1e9\n",
        "\n",
        "e.g. in python with numpy and SFrames:\n",
        "\n",
        "      model_features = ['sqft_living', 'sqft_living15']\n",
        "      my_output = 'price'\n",
        "     (feature_matrix, output) = get_numpy_data(train_data, model_features,my_output)\n",
        "      initial_weights = np.array([-100000., 1., 1.])\n",
        "      step_size = 4e-12\n",
        "      tolerance = 1e9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxBYysNNF_Cg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_features = ['sqft_living', 'sqft_living15']\n",
        "my_output = 'price'\n",
        "(feature_matrix, output) = get_numpy_data(train_data, model_features,my_output)\n",
        "initial_model_weights = np.array([-100000., 1., 1.])\n",
        "step_size = 4e-12\n",
        "tolerance = 1e9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPaXW3YfGgSE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0334dc6-8556-473d-cbda-82b768a309d9"
      },
      "source": [
        "model_weights = regression_gradient_descent(feature_matrix, output, initial_model_weights, step_size, tolerance)\n",
        "print(model_weights)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-9.99999688e+04  2.45072603e+02  6.52795277e+01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIKUGBgRHMsT",
        "colab_type": "text"
      },
      "source": [
        "14. Use the regression weights from this second model (using sqft_living and sqft_living_15) and predict the outcome of all the house prices on the TEST data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkciUQjBHEQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(test_feature_matrix, model_output) = get_numpy_data(test_data, model_features, my_output) \n",
        "model_test_predictions = predict_output(test_feature_matrix, model_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8kee0mZHpAQ",
        "colab_type": "text"
      },
      "source": [
        "#15. **Quiz Question:** What is the predicted price for the 1st house in the TEST data set for model 2 (round to nearest dollar)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEQbk0EyHdqJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "342aa10e-7fcb-43ce-971b-ddb77afa676d"
      },
      "source": [
        "print(np.rint(model_test_predictions[0]))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "366651.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfsW_7BPHxOg",
        "colab_type": "text"
      },
      "source": [
        "16. What is the actual price for the 1st house in the Test data set?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxHdjabhHiGI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9af6d061-53ff-4314-9913-8fe692bb539d"
      },
      "source": [
        "print(test_data[0]['price'])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "310000.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfVaZ9BCH8w9",
        "colab_type": "text"
      },
      "source": [
        "#17. **Quiz Question:** Which estimate was closer to the true price for the 1st house on the TEST data set, model 1 or model 2?\n",
        "Model 1 is nearer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWnG8RQ5IS8l",
        "colab_type": "text"
      },
      "source": [
        "18. Now compute RSS on all test data for the second model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSMZQRb2H38T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ce5f7da-3bd8-4f77-db57-0f15f5c13e1a"
      },
      "source": [
        "model_errors = model_test_predictions - model_output\n",
        "RSS2 = (model_errors*model_errors).sum()\n",
        "print('RSS for the 2nd model : ',RSS2)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RSS for the 2nd model :  270263446465244.06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KPzTK5zI75e",
        "colab_type": "text"
      },
      "source": [
        "#19. **Quiz Question:** Which model (1 or 2) has lowest RSS on all of the TEST data?\n",
        "model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcUq-z72Iu3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}